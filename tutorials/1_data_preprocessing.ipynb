{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7655159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from types import SimpleNamespace\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02e0837",
   "metadata": {},
   "source": [
    "In this tutorial we will go through the steps to prepare data for input to GHIST. We will use a subset of a dataset from 10x Genomics (Breast Cancer Sample 2 https://www.10xgenomics.com/products/xenium-in-situ/preview-dataset-human-breast). We also provide all the processed data to run GHIST for this dataset in https://drive.google.com/drive/folders/1ebk_2l9dUzT35xgRHMtt3SMSi2rWpVY0?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9113d53a",
   "metadata": {},
   "source": [
    "## Setup - Download data\n",
    "\n",
    "#### 1. Download the Xenium data subset\n",
    "\n",
    "Note we provide the H&E image that is aligned to the spatial transcriptomics data for this dataset. This tutorial does not provide the code to align H&E images with Xenium data. Affine transform parameters are available for some datasets. Alternatively, please check out https://www.10xgenomics.com/analysis-guides/he-to-xenium-dapi-image-registration-with-fiji. Manual registration is between H&E and DAPI images is another option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cad8599f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1HPP_gHdZ8TjDMpMiZOOjmGbBtgkp0RCK\n",
      "From (redirected): https://drive.google.com/uc?id=1HPP_gHdZ8TjDMpMiZOOjmGbBtgkp0RCK&confirm=t&uuid=501d3248-1b3c-4ea9-b5b8-9c32b5b78554\n",
      "To: /dskh/nobackup/helenf/project_GHIST/_GITHUB_restructure_branch/tutorials/data_processing_demo.zip\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 69.6M/69.6M [00:02<00:00, 29.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown https://drive.google.com/uc?id=1HPP_gHdZ8TjDMpMiZOOjmGbBtgkp0RCK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95611f11",
   "metadata": {},
   "source": [
    "#### 2. Extract the data bundle\n",
    "\n",
    "The following files from the Xenium bundle are required :\n",
    "- nucleus_boundaries.csv.gz\n",
    "- cell_feature_matrix (sometimes it is compressed (e.g., cell_feature_matrix.tar.gz), please ensure the folder is extracted)\n",
    "- H&E image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f0a42ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs_dir = \"data_processing_demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85496c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data_processing_demo.zip\n",
      "  inflating: data_processing_demo/nucleus_boundaries.csv.gz  \n",
      "  inflating: data_processing_demo/cell_feature_matrix.tar.gz  \n",
      "  inflating: data_processing_demo/he_image.tif  \n"
     ]
    }
   ],
   "source": [
    "!unzip {outs_dir + \".zip\"} -d {outs_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2b60de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell_feature_matrix extracted\n"
     ]
    }
   ],
   "source": [
    "tar_path = os.path.join(outs_dir, \"cell_feature_matrix.tar.gz\")\n",
    "extracted_folder = os.path.join(outs_dir, \"cell_feature_matrix\")\n",
    "\n",
    "if os.path.exists(extracted_folder):\n",
    "    print(f\"cell_feature_matrix folder already exists\")\n",
    "elif os.path.exists(tar_path):\n",
    "    with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=outs_dir)\n",
    "    print(\"cell_feature_matrix extracted\")\n",
    "else:\n",
    "    print(\"Neither 'cell_feature_matrix.tar.gz' nor the extracted folder were found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3352b7",
   "metadata": {},
   "source": [
    "#### 3. Clone the Hover-Net GitHub repository and follow installation steps\n",
    "\n",
    "In this example data processing code we are using Hover-Net (Graham et al., Medical Image Analysis 2019) to segment nuclei in H&E images. Please clone their repository (https://github.com/vqdang/hover_net) and install their environment (separate to the environment for GHIST). Download their checkpoint `hovernet_original_consep_notype_tf2pytorch.tar` and place in the main `hover_net` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fcc76a",
   "metadata": {},
   "source": [
    "## Setup - Parameters\n",
    "\n",
    "Here, we define locations of data directories and files, and other parameters.\n",
    "\n",
    "- ``code_dir``: path to the ``data_processing`` folder in this repo\n",
    "- ``dir_output``: this is the directory that will be created where processed data will be saved\n",
    "- ``dir_xenium_outs``: path to the extracted Xenium output bundle folder\n",
    "- ``fp_he_img``: path to the H&E image file that is aligned to the Xenium data (``.ome.tif`` or ``.tif`` file)\n",
    "- ``dir_hovernet``: path to the Hover-Net code (folder should be ``hover_net``)\n",
    "- ``gpu_id``: which GPU to use\n",
    "- ``n_processes``: how many CPUs to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9acf1a6",
   "metadata": {},
   "source": [
    "<span style=\"color: red; font-weight: bold\">Please update the following parameters:</span>\n",
    "\n",
    "The key parameter to update for this tutorial is `dir_hovernet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70d6c424",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SimpleNamespace(\n",
    "    code_dir=\"../data_processing\",\n",
    "    dir_output=\"../tutorials/data_processing_demo_outputs\",\n",
    "    dir_xenium_outs=\"../tutorials/data_processing_demo\",\n",
    "    fp_he_img=\"../tutorials/data_processing_demo/he_image.tif\",\n",
    "    dir_hovernet=\"/dskh/nobackup/helenf/hover_net\",\n",
    "    gpu_id=0,\n",
    "    n_processes=24\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "147aa80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change working directory\n",
    "os.chdir(config.code_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae8fc82",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "\n",
    "Get the nuclei segmentation image for Xenium data. This step extracts from nuclei the boundaries file in the data bundle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c8e1f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 907/907 [00:04<00:00, 192.54it/s]]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1124/1124 [00:05<00:00, 206.86it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1092/1092 [00:05<00:00, 193.64it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1184/1184 [00:05<00:00, 200.55it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1134/1134 [00:06<00:00, 183.85it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1322/1322 [00:06<00:00, 196.63it/s]\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:00<00:00, 23.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining crops\n",
      "Total nuclei 5410\n",
      "Deleting intermediate files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 28.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "command = f\"\"\"\n",
    "python 1_get_xenium_nuclei_seg_image.py \\\n",
    "    --fp_boundaries {config.dir_xenium_outs}/nucleus_boundaries.csv.gz \\\n",
    "    --dir_output {config.dir_output} \\\n",
    "    --fp_he_img {config.fp_he_img} \\\n",
    "    --n_processes {config.n_processes}\n",
    "\"\"\"\n",
    "os.system(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e97ac3",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "\n",
    "Get the cell gene matrix for the Xenium data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a334eec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../tutorials/data_processing_demo/cell_feature_matrix/barcodes.tsv\n",
      "Loading ../tutorials/data_processing_demo/cell_feature_matrix/features.tsv\n",
      "Saved ../tutorials/data_processing_demo_outputs/cell_gene_matrix.csv\n",
      "Deleting intermediate files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "command = f\"\"\"\n",
    "python 2_get_xenium_cell_gene_matrix.py \\\n",
    "    --dir_feature_matrix {config.dir_xenium_outs}/cell_feature_matrix \\\n",
    "    --dir_output {config.dir_output}\n",
    "\"\"\"\n",
    "os.system(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af2b25e",
   "metadata": {},
   "source": [
    "## Step 3.1\n",
    "\n",
    "(H&E nuclei segmentation) - divide the H&E image in to patches for nuclei segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f03900f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done cropping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "command = f\"\"\"\n",
    "python 3_segment_nuclei_he_image.py \\\n",
    "    --dir_output {config.dir_output} \\\n",
    "    --fp_he_img {config.fp_he_img} \\\n",
    "    --dir_hovernet {config.dir_hovernet} \\\n",
    "    --gpu_id {config.gpu_id} \\\n",
    "    --step 1\n",
    "\"\"\"\n",
    "os.system(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39324744",
   "metadata": {},
   "source": [
    "## Step 3.2\n",
    "\n",
    "(H&E nuclei segmentation) - segment the nuclei using Hover-Net.\n",
    "\n",
    "During this step, we have encountered the error below, though the segmentation still ran fine:\n",
    "``Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library. Try to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c35b810a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.\n",
      "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
      "|2025-05-09|08:41:30.276| [INFO] .... Detect #GPUS: 1\n",
      "Process Patches:   0%|                                   | 0/10 [00:00<?, ?it/s]Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.\n",
      "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
      "Process Patches: 100%|##########################| 10/10 [00:34<00:00,  3.50s/it]\n",
      "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.\n",
      "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
      "|2025-05-09|08:42:22.354| [INFO] ........................ Done Assembling 0_0\n",
      "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.\n",
      "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
      "|2025-05-09|08:42:31.314| [INFO] .... Detect #GPUS: 1\n",
      "Process Patches:   0%|                                   | 0/10 [00:00<?, ?it/s]Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.\n",
      "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
      "Process Patches: 100%|##########################| 10/10 [00:32<00:00,  3.26s/it]\n",
      "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.\n",
      "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
      "|2025-05-09|08:43:17.225| [INFO] ........................ Done Assembling 0_2000\n",
      "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.\n",
      "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
      "|2025-05-09|08:43:25.135| [INFO] .... Detect #GPUS: 1\n",
      "Process Patches:   0%|                                   | 0/10 [00:00<?, ?it/s]Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.\n",
      "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
      "Process Patches: 100%|##########################| 10/10 [00:35<00:00,  3.55s/it]\n",
      "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.\n",
      "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
      "|2025-05-09|08:44:17.280| [INFO] ........................ Done Assembling 2000_0\n",
      "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.\n",
      "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
      "|2025-05-09|08:44:25.625| [INFO] .... Detect #GPUS: 1\n",
      "Process Patches:   0%|                                   | 0/10 [00:00<?, ?it/s]Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.\n",
      "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
      "Process Patches: 100%|##########################| 10/10 [00:36<00:00,  3.63s/it]\n",
      "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.\n",
      "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
      "|2025-05-09|08:45:14.152| [INFO] ........................ Done Assembling 2000_2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make sure you are using the correct env for hovernet\n",
      "/dskh/nobackup/helenf/project_GHIST/_GITHUB_restructure_branch/tutorials/data_processing_demo_outputs/he_image_nuclei_seg_crops\n",
      "/dskh/nobackup/helenf/project_GHIST/_GITHUB_restructure_branch/tutorials/data_processing_demo_outputs/he_image_nuclei_seg_crops_hovernet\n",
      "Num crops found: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='\\nsource /dskh/nobackup/helenf/miniconda3/etc/profile.d/conda.sh && conda activate hovernet && python 3_segment_nuclei_he_image.py   --dir_output ../tutorials/data_processing_demo_outputs   --fp_he_img ../tutorials/data_processing_demo/he_image.tif   --dir_hovernet /dskh/nobackup/helenf/hover_net   --gpu_id 0   --step 2\\n', returncode=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conda_base = subprocess.check_output(\"conda info --base\", shell=True).decode().strip()\n",
    "\n",
    "cmd = f\"\"\"\n",
    "source {conda_base}/etc/profile.d/conda.sh && \\\n",
    "conda activate hovernet && \\\n",
    "python 3_segment_nuclei_he_image.py \\\n",
    "  --dir_output {config.dir_output} \\\n",
    "  --fp_he_img {config.fp_he_img} \\\n",
    "  --dir_hovernet {config.dir_hovernet} \\\n",
    "  --gpu_id {config.gpu_id} \\\n",
    "  --step 2\n",
    "\"\"\"\n",
    "\n",
    "subprocess.run(cmd, shell=True, executable=\"/bin/bash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33538ccb",
   "metadata": {},
   "source": [
    "If the above doesn't work, please open up a terminal in the `GHIST/data_processing` folder and run:\n",
    "\n",
    "```sh\n",
    "conda activate hovernet\n",
    "\n",
    "python 3_segment_nuclei_he_image.py --dir_output data_processing_demo_outputs --fp_he_img ../tutorials/data_processing_demo/he_image.tif --dir_hovernet {/path/to/hover_net} --gpu_id 0 --step 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdbe9c1",
   "metadata": {},
   "source": [
    "## Step 3.3\n",
    "\n",
    "(H&E nuclei segmentation) - combine the nuclei from patches to a whole image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ababf9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num IDs before combining: 5891\n",
      "Combining crops...\n",
      "Num nuclei intermediate step 5870\n",
      "Num nuclei final: 5844\n",
      "Saved ../tutorials/data_processing_demo_outputs/he_image_nuclei_seg.tif\n",
      "Num nuclei final: 5840\n",
      "Deleting intermediate files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 135.52it/s]\n",
      "\r\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 68.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='\\nsource /dskh/nobackup/helenf/miniconda3/etc/profile.d/conda.sh && conda activate ghist && python 3_segment_nuclei_he_image.py   --dir_output ../tutorials/data_processing_demo_outputs   --fp_he_img ../tutorials/data_processing_demo/he_image.tif   --dir_hovernet /dskh/nobackup/helenf/hover_net   --gpu_id 0   --step 3\\n', returncode=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conda_base = subprocess.check_output(\"conda info --base\", shell=True).decode().strip()\n",
    "\n",
    "cmd = f\"\"\"\n",
    "source {conda_base}/etc/profile.d/conda.sh && \\\n",
    "conda activate ghist && \\\n",
    "python 3_segment_nuclei_he_image.py \\\n",
    "  --dir_output {config.dir_output} \\\n",
    "  --fp_he_img {config.fp_he_img} \\\n",
    "  --dir_hovernet {config.dir_hovernet} \\\n",
    "  --gpu_id {config.gpu_id} \\\n",
    "  --step 3\n",
    "\"\"\"\n",
    "\n",
    "subprocess.run(cmd, shell=True, executable=f\"/bin/bash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67d4f94",
   "metadata": {},
   "source": [
    "If the above doesn't work, please open up a terminal in the `GHIST/data_processing` folder and run:\n",
    "\n",
    "```sh\n",
    "conda activate ghist \n",
    "\n",
    "python 3_segment_nuclei_he_image.py --dir_output data_processing_demo_outputs --fp_he_img ../tutorials/data_processing_demo/he_image.tif --dir_hovernet {/path/to/hover_net} --gpu_id 0 --step 3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ca4b60",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "\n",
    "Determine corresponding cells between H&E images and Xenium data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ce7da41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing using CPUs: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5841/5841 [00:00<00:00, 9120.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ../tutorials/data_processing_demo_outputs/cell_gene_matrix_filtered.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "command = f\"\"\"\n",
    "python 4_get_corresponding_cells.py \\\n",
    "    --dir_output {config.dir_output} \\\n",
    "    --n_processes {config.n_processes}\n",
    "\"\"\"\n",
    "os.system(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4335642a",
   "metadata": {},
   "source": [
    "ðŸ“Œ At this point the following files should in the directory ``dir_output``:\n",
    "\n",
    "- ``cell_gene_matrix.csv`` - cell gene matrix for all cells in the Xenium data\n",
    "- *``cell_gene_matrix_filtered.csv`` - cell gene matrix only for filtered cells\n",
    "- ``genes.txt`` - list of genes in the panel\n",
    "- *``he_image_nuclei_seg.tif`` - nuclei segmented from the H&E image\n",
    "- ``he_image_nuclei_seg_microns.tif`` - nuclei segmented from the H&E image resized to 1 pixel = 1 micron\n",
    "- ``matched_nuclei.csv`` - corresponding cell IDs between H&E and Xenium\n",
    "- *``matched_nuclei_filtered.csv`` - corresponding cell IDs of filtered cells\n",
    "- ``xenium_cell_ids_dict.csv`` - created only if Xenium cell IDs are strings\n",
    "- ``xenium_nuclei.tif`` - nuclei segmented from Xenium data\n",
    "\n",
    "(*) marks files needed to run GHIST (see ``data_demo`` as an example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee1bccf",
   "metadata": {},
   "source": [
    "## Optional - Cell type\n",
    "\n",
    "Cell type annotation - if you want to use cell type or neighbourhood information, run your preferred cell annotation method on ``cell_gene_matrix_filtered.csv`` to get cell type labels. Then create a csv file with columns like the following (e.g., see ``data_demo/celltype_filtered.csv``):\n",
    "\n",
    "```\n",
    "c_id,ct\n",
    "1,T\n",
    "2,Malignant\n",
    "3,Macrophage\n",
    "...etc\n",
    "```\n",
    "\n",
    "Number of cell types: based on our experiments we recommend using <10 cell types\n",
    "\n",
    "ðŸ“Œ If you do not wish to use cell type information, in your config file:\n",
    "\n",
    "- ``\"celltype\": false,``\n",
    "- delete parameter `fp_cell_type` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3420716",
   "metadata": {},
   "source": [
    "## Optional - AvgExp\n",
    "\n",
    "Averaged single cell gene expression profiles of cell types - this is an optional input. These profiles do not need to be matched for the same sample of interest, but should ideally be for the same tissue type (e.g. breast cancer). AvgExp can have a flexible number and categories of cell types from a single or multiple reference datasets. The same AvgExp data should be used during training and inference.\n",
    "\n",
    "ðŸ“Œ If you do not wish to use AvgExp, in your config file:\n",
    "\n",
    "- ``\"avgexp\": false,``\n",
    "- delete parameter `fp_avgexp` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0d3543",
   "metadata": {},
   "source": [
    "# New territory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe41e2f",
   "metadata": {},
   "source": [
    "## Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e1c083",
   "metadata": {},
   "source": [
    "Create variant data with varseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b31a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading fastq files (125GB for .fastq files)\n",
      "Running prefetch\n",
      "prefetch --progress --verbose SRR26065624 SRR26065625 SRR26065626 SRR26065627 SRR26065628 SRR26065629 SRR26065630 SRR26065631 SRR26065632 SRR26065633 SRR26065634 SRR26065635 SRR26065636 SRR26065637 SRR26065638 SRR26065639\n",
      "Downloading files\n",
      "fasterq-dump --outdir ../data_demo/variant_data/sequencing_data_raw --threads 24 --progress --verbose --split-files SRR26065624 SRR26065625 SRR26065626 SRR26065627 SRR26065628 SRR26065629 SRR26065630 SRR26065631 SRR26065632 SRR26065633 SRR26065634 SRR26065635 SRR26065636 SRR26065637 SRR26065638 SRR26065639\n",
      "Fastq data downloaded successfully to ../data_demo/variant_data/sequencing_data_raw\n"
     ]
    }
   ],
   "source": [
    "dry_run = True\n",
    "\n",
    "if not os.path.exists(config.dir_output):\n",
    "    config.dir_output = \"../data_demo\"  # only if I am jumping straight here\n",
    "\n",
    "variant_data_dir = os.path.join(config.dir_output, \"variant_data\")\n",
    "if not os.path.exists(variant_data_dir):\n",
    "    os.makedirs(variant_data_dir)\n",
    "\n",
    "GEO_url = \"https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE243280\"\n",
    "SRA_url = \"https://www.ncbi.nlm.nih.gov/Traces/study/?acc=PRJNA1017511&f=librarysource_s%3An%3Atranscriptomic%2520single%2520cell&o=acc_s%3Aa&s=SRR26065624,SRR26065625,SRR26065626,SRR26065627,SRR26065628,SRR26065629,SRR26065630,SRR26065631,SRR26065632,SRR26065633,SRR26065634,SRR26065635,SRR26065636,SRR26065637,SRR26065638,SRR26065639\"\n",
    "SRA_accession_file = os.path.join(variant_data_dir, \"SRR_Acc_List_scrnaseq_tutorial.txt\")  #!!! this is wrong - replace with new\n",
    "\n",
    "sequencing_data_raw_dir = os.path.join(variant_data_dir, \"sequencing_data_raw\")\n",
    "if not os.path.exists(sequencing_data_raw_dir) or len(os.listdir(sequencing_data_raw_dir)) == 0:\n",
    "    if not os.path.exists(SRA_accession_file):\n",
    "        raise ValueError(f\"Please download the SRA accession file and save it as {SRA_accession_file}\\nSRA url: {SRA_url}\\nGEO url: {GEO_url}\")\n",
    "    os.makedirs(sequencing_data_raw_dir, exist_ok=True)\n",
    "\n",
    "    # check for fastqs\n",
    "    with open(SRA_accession_file) as f:\n",
    "        srr_list = f.read().split()\n",
    "    try:\n",
    "        print(\"Downloading fastq files (152GB for .fastq files)\")\n",
    "        \n",
    "        print(\"Running prefetch\")\n",
    "        prefetch_command = [\"prefetch\", \"--progress\", \"--verbose\"] + srr_list\n",
    "        print(\" \".join(prefetch_command))\n",
    "        if not dry_run:\n",
    "            subprocess.run(prefetch_command, check=True)\n",
    "        \n",
    "        print(\"Downloading files\")\n",
    "        data_download_command = [\"fasterq-dump\", \"--outdir\", os.path.abspath(sequencing_data_raw_dir), \"--threads\", str(config.n_processes), \"--progress\", \"--verbose\", \"--split-files\"] + srr_list\n",
    "        print(\" \".join(data_download_command))\n",
    "        if not dry_run:\n",
    "            subprocess.run(data_download_command, check=True)\n",
    "        \n",
    "        print(f\"Fastq data downloaded successfully to {sequencing_data_raw_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error running fasterq-dump: {e}\")\n",
    "        raise  # re-raises the original exception\n",
    "else:\n",
    "    print(f\"Fastq data already exists in {sequencing_data_raw_dir}.\")\n",
    "\n",
    "varseek_cosmic_cmc_index_url = \"https://caltech.box.com/shared/static/f58qvvmp6en4dha28d7pakafg8j9jzqi.idx\"\n",
    "varseek_cosmic_cmc_t2g_url = \"https://caltech.box.com/shared/static/taqo1cyswui69rdiy3jqpxdtsgbnin10.txt\"\n",
    "vk_ref_dir = os.path.join(variant_data_dir, \"vk_ref_out\")\n",
    "idx_path = os.path.join(vk_ref_dir, \"cosmic_cmc_index.idx\")\n",
    "t2g_path = os.path.join(vk_ref_dir, \"cosmic_cmc_t2g.txt\")\n",
    "\n",
    "os.makedirs(vk_ref_dir, exist_ok=True)    \n",
    "if not os.path.exists(idx_path):\n",
    "    subprocess.run(f\"wget {varseek_cosmic_cmc_index_url} -O {idx_path}\", shell=True, check=True)\n",
    "if not os.path.exists(t2g_path):\n",
    "    subprocess.run(f\"wget {varseek_cosmic_cmc_t2g_url} -O {t2g_path}\", shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166034f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "technology = \"10XV3\"\n",
    "k = 51  #* don't change unless I change my vk ref index settings\n",
    "min_counts = 3\n",
    "use_binary_matrix = True\n",
    "drop_empty_columns = True\n",
    "\n",
    "command = f\"\"\"\n",
    "python 5_get_variant_data.py \\\n",
    "    --dir_output {config.dir_output} \\\n",
    "    --technology {technology} \\\n",
    "    --k {k} \\\n",
    "    --min_counts {min_counts} \\\n",
    "    --disable_use_binary_matrix {not use_binary_matrix} \\\n",
    "    --disable_drop_empty_columns {not drop_empty_columns} \\\n",
    "    --n_processes {config.n_processes} \\\n",
    "    --variant_data_dir {variant_data_dir} \\\n",
    "    --fastqs_dir {sequencing_data_raw_dir} \\\n",
    "    --vk_ref_dir {vk_ref_dir} \\\n",
    "    --index {idx_path} \\\n",
    "    --t2g {t2g_path}\n",
    "\"\"\"\n",
    "os.system(command)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ghist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
